{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9846091,"sourceType":"datasetVersion","datasetId":6041062}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1) Data Loading & Pre-processing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_excel(\"/kaggle/input/nlp-project-dataset/Main.xlsx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:22.242325Z","iopub.execute_input":"2024-11-27T19:33:22.242832Z","iopub.status.idle":"2024-11-27T19:33:26.108019Z","shell.execute_reply.started":"2024-11-27T19:33:22.242777Z","shell.execute_reply":"2024-11-27T19:33:26.107257Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.109353Z","iopub.execute_input":"2024-11-27T19:33:26.109723Z","iopub.status.idle":"2024-11-27T19:33:26.133072Z","shell.execute_reply.started":"2024-11-27T19:33:26.109695Z","shell.execute_reply":"2024-11-27T19:33:26.132270Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  Batch Source Language  ID  Type  \\\n0   B01         English   1  MAIN   \n1   B01         English   2  MAIN   \n2   B01         English   4  MAIN   \n3   B01         English   7  MAIN   \n4   B01         English   8  MAIN   \n\n                                                Text  \\\n0  Yemen's Houthis have waded into the Israel-Ham...   \n1             Isreal - Hamas Conflict | Face to Face   \n2  Videos show how armed men from Gaza stormed a ...   \n3  Protest in Aligarh Muslim University in suppor...   \n4  IDF releases audio recording about misfired ro...   \n\n                                          English MT  \\\n0  Yemen's Houthis have waded into the Israel-Ham...   \n1             Isreal - Hamas Conflict | Face to Face   \n2  Videos show how armed men from Gaza stormed a ...   \n3  Protest in Aligarh Muslim University in suppor...   \n4  IDF releases audio recording about misfired ro...   \n\n                                           Arabic MT  Annotator ID  \\\n0  خاض الحوثيون في اليمن الحرب بين إسرائيل وحماس ...           1.0   \n1               إسرائيل - الصراع مع حماس | وجها لوجه           4.0   \n2  أظهرت مقاطع فيديو كيف اقتحم مسلحون من غزة مهرج...           3.0   \n3  وقفة احتجاجية في جامعة عليكرة الإسلامية دعما ل...           5.0   \n4  الجيش الإسرائيلي ينشر تسجيلًا صوتيًا حول صاروخ...           2.0   \n\n                       Bias      Propaganda            Type of Propaganda  \\\n0  Biased against Palestine  Not Propaganda  Propaganda Not to be deleted   \n1                  Unbiased  Not Propaganda                Not Propaganda   \n2                  Unbiased  Not Propaganda  Propaganda Not to be deleted   \n3                  Unbiased  Not Propaganda                Not Propaganda   \n4  Biased against Palestine      Propaganda    Propaganda Must be deleted   \n\n  Type of Bias  Comments  \n0         ضمني       NaN  \n1          NaN       NaN  \n2          NaN       NaN  \n3          NaN       NaN  \n4         ضمني       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Batch</th>\n      <th>Source Language</th>\n      <th>ID</th>\n      <th>Type</th>\n      <th>Text</th>\n      <th>English MT</th>\n      <th>Arabic MT</th>\n      <th>Annotator ID</th>\n      <th>Bias</th>\n      <th>Propaganda</th>\n      <th>Type of Propaganda</th>\n      <th>Type of Bias</th>\n      <th>Comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B01</td>\n      <td>English</td>\n      <td>1</td>\n      <td>MAIN</td>\n      <td>Yemen's Houthis have waded into the Israel-Ham...</td>\n      <td>Yemen's Houthis have waded into the Israel-Ham...</td>\n      <td>خاض الحوثيون في اليمن الحرب بين إسرائيل وحماس ...</td>\n      <td>1.0</td>\n      <td>Biased against Palestine</td>\n      <td>Not Propaganda</td>\n      <td>Propaganda Not to be deleted</td>\n      <td>ضمني</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B01</td>\n      <td>English</td>\n      <td>2</td>\n      <td>MAIN</td>\n      <td>Isreal - Hamas Conflict | Face to Face</td>\n      <td>Isreal - Hamas Conflict | Face to Face</td>\n      <td>إسرائيل - الصراع مع حماس | وجها لوجه</td>\n      <td>4.0</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Not Propaganda</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B01</td>\n      <td>English</td>\n      <td>4</td>\n      <td>MAIN</td>\n      <td>Videos show how armed men from Gaza stormed a ...</td>\n      <td>Videos show how armed men from Gaza stormed a ...</td>\n      <td>أظهرت مقاطع فيديو كيف اقتحم مسلحون من غزة مهرج...</td>\n      <td>3.0</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Propaganda Not to be deleted</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B01</td>\n      <td>English</td>\n      <td>7</td>\n      <td>MAIN</td>\n      <td>Protest in Aligarh Muslim University in suppor...</td>\n      <td>Protest in Aligarh Muslim University in suppor...</td>\n      <td>وقفة احتجاجية في جامعة عليكرة الإسلامية دعما ل...</td>\n      <td>5.0</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Not Propaganda</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B01</td>\n      <td>English</td>\n      <td>8</td>\n      <td>MAIN</td>\n      <td>IDF releases audio recording about misfired ro...</td>\n      <td>IDF releases audio recording about misfired ro...</td>\n      <td>الجيش الإسرائيلي ينشر تسجيلًا صوتيًا حول صاروخ...</td>\n      <td>2.0</td>\n      <td>Biased against Palestine</td>\n      <td>Propaganda</td>\n      <td>Propaganda Must be deleted</td>\n      <td>ضمني</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.134088Z","iopub.execute_input":"2024-11-27T19:33:26.134363Z","iopub.status.idle":"2024-11-27T19:33:26.139703Z","shell.execute_reply.started":"2024-11-27T19:33:26.134336Z","shell.execute_reply":"2024-11-27T19:33:26.138852Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index(['Batch', 'Source Language', 'ID', 'Type', 'Text', 'English MT',\n       'Arabic MT', 'Annotator ID', 'Bias', 'Propaganda', 'Type of Propaganda',\n       'Type of Bias', 'Comments'],\n      dtype='object')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df = df.drop(columns=['Annotator ID', 'Text','Batch', 'ID', 'Comments', 'Type', 'Source Language', 'English MT',\"Type of Bias\"], errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.141715Z","iopub.execute_input":"2024-11-27T19:33:26.142028Z","iopub.status.idle":"2024-11-27T19:33:26.153576Z","shell.execute_reply.started":"2024-11-27T19:33:26.141966Z","shell.execute_reply":"2024-11-27T19:33:26.152925Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.154482Z","iopub.execute_input":"2024-11-27T19:33:26.154745Z","iopub.status.idle":"2024-11-27T19:33:26.170457Z","shell.execute_reply.started":"2024-11-27T19:33:26.154721Z","shell.execute_reply":"2024-11-27T19:33:26.169455Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                           Arabic MT  \\\n0  خاض الحوثيون في اليمن الحرب بين إسرائيل وحماس ...   \n1               إسرائيل - الصراع مع حماس | وجها لوجه   \n2  أظهرت مقاطع فيديو كيف اقتحم مسلحون من غزة مهرج...   \n3  وقفة احتجاجية في جامعة عليكرة الإسلامية دعما ل...   \n4  الجيش الإسرائيلي ينشر تسجيلًا صوتيًا حول صاروخ...   \n\n                       Bias      Propaganda            Type of Propaganda  \n0  Biased against Palestine  Not Propaganda  Propaganda Not to be deleted  \n1                  Unbiased  Not Propaganda                Not Propaganda  \n2                  Unbiased  Not Propaganda  Propaganda Not to be deleted  \n3                  Unbiased  Not Propaganda                Not Propaganda  \n4  Biased against Palestine      Propaganda    Propaganda Must be deleted  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Arabic MT</th>\n      <th>Bias</th>\n      <th>Propaganda</th>\n      <th>Type of Propaganda</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>خاض الحوثيون في اليمن الحرب بين إسرائيل وحماس ...</td>\n      <td>Biased against Palestine</td>\n      <td>Not Propaganda</td>\n      <td>Propaganda Not to be deleted</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>إسرائيل - الصراع مع حماس | وجها لوجه</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Not Propaganda</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>أظهرت مقاطع فيديو كيف اقتحم مسلحون من غزة مهرج...</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Propaganda Not to be deleted</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>وقفة احتجاجية في جامعة عليكرة الإسلامية دعما ل...</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Not Propaganda</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>الجيش الإسرائيلي ينشر تسجيلًا صوتيًا حول صاروخ...</td>\n      <td>Biased against Palestine</td>\n      <td>Propaganda</td>\n      <td>Propaganda Must be deleted</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"total_rows = len(df)\nprint(f\"Total number of rows: {total_rows}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.171590Z","iopub.execute_input":"2024-11-27T19:33:26.171912Z","iopub.status.idle":"2024-11-27T19:33:26.180322Z","shell.execute_reply.started":"2024-11-27T19:33:26.171876Z","shell.execute_reply":"2024-11-27T19:33:26.179625Z"}},"outputs":[{"name":"stdout","text":"Total number of rows: 13500\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.181190Z","iopub.execute_input":"2024-11-27T19:33:26.181415Z","iopub.status.idle":"2024-11-27T19:33:26.194320Z","shell.execute_reply.started":"2024-11-27T19:33:26.181392Z","shell.execute_reply":"2024-11-27T19:33:26.193551Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Arabic MT                0\nBias                  2700\nPropaganda            2700\nType of Propaganda    2700\ndtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.195444Z","iopub.execute_input":"2024-11-27T19:33:26.195767Z","iopub.status.idle":"2024-11-27T19:33:26.202394Z","shell.execute_reply.started":"2024-11-27T19:33:26.195734Z","shell.execute_reply":"2024-11-27T19:33:26.201743Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.replace(['', 'None', 'nan'], np.nan, inplace=True)  \ndf.dropna(subset=['Bias', 'Propaganda', 'Type of Propaganda'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.203230Z","iopub.execute_input":"2024-11-27T19:33:26.203469Z","iopub.status.idle":"2024-11-27T19:33:26.224743Z","shell.execute_reply.started":"2024-11-27T19:33:26.203446Z","shell.execute_reply":"2024-11-27T19:33:26.224220Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.228152Z","iopub.execute_input":"2024-11-27T19:33:26.228369Z","iopub.status.idle":"2024-11-27T19:33:26.236666Z","shell.execute_reply.started":"2024-11-27T19:33:26.228347Z","shell.execute_reply":"2024-11-27T19:33:26.235765Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Arabic MT             0\nBias                  0\nPropaganda            0\nType of Propaganda    0\ndtype: int64"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.237725Z","iopub.execute_input":"2024-11-27T19:33:26.237988Z","iopub.status.idle":"2024-11-27T19:33:26.248900Z","shell.execute_reply.started":"2024-11-27T19:33:26.237965Z","shell.execute_reply":"2024-11-27T19:33:26.247910Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                           Arabic MT  \\\n0  خاض الحوثيون في اليمن الحرب بين إسرائيل وحماس ...   \n1               إسرائيل - الصراع مع حماس | وجها لوجه   \n2  أظهرت مقاطع فيديو كيف اقتحم مسلحون من غزة مهرج...   \n3  وقفة احتجاجية في جامعة عليكرة الإسلامية دعما ل...   \n4  الجيش الإسرائيلي ينشر تسجيلًا صوتيًا حول صاروخ...   \n\n                       Bias      Propaganda            Type of Propaganda  \n0  Biased against Palestine  Not Propaganda  Propaganda Not to be deleted  \n1                  Unbiased  Not Propaganda                Not Propaganda  \n2                  Unbiased  Not Propaganda  Propaganda Not to be deleted  \n3                  Unbiased  Not Propaganda                Not Propaganda  \n4  Biased against Palestine      Propaganda    Propaganda Must be deleted  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Arabic MT</th>\n      <th>Bias</th>\n      <th>Propaganda</th>\n      <th>Type of Propaganda</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>خاض الحوثيون في اليمن الحرب بين إسرائيل وحماس ...</td>\n      <td>Biased against Palestine</td>\n      <td>Not Propaganda</td>\n      <td>Propaganda Not to be deleted</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>إسرائيل - الصراع مع حماس | وجها لوجه</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Not Propaganda</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>أظهرت مقاطع فيديو كيف اقتحم مسلحون من غزة مهرج...</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Propaganda Not to be deleted</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>وقفة احتجاجية في جامعة عليكرة الإسلامية دعما ل...</td>\n      <td>Unbiased</td>\n      <td>Not Propaganda</td>\n      <td>Not Propaganda</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>الجيش الإسرائيلي ينشر تسجيلًا صوتيًا حول صاروخ...</td>\n      <td>Biased against Palestine</td>\n      <td>Propaganda</td>\n      <td>Propaganda Must be deleted</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"total_rows = len(df)\nprint(f\"Total number of rows: {total_rows}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.249750Z","iopub.execute_input":"2024-11-27T19:33:26.249947Z","iopub.status.idle":"2024-11-27T19:33:26.258362Z","shell.execute_reply.started":"2024-11-27T19:33:26.249927Z","shell.execute_reply":"2024-11-27T19:33:26.257628Z"}},"outputs":[{"name":"stdout","text":"Total number of rows: 10800\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(df['Bias'].cat.categories if df['Bias'].dtype.name == 'category' else df['Bias'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.259281Z","iopub.execute_input":"2024-11-27T19:33:26.259562Z","iopub.status.idle":"2024-11-27T19:33:26.271971Z","shell.execute_reply.started":"2024-11-27T19:33:26.259528Z","shell.execute_reply":"2024-11-27T19:33:26.271048Z"}},"outputs":[{"name":"stdout","text":"['Biased against Palestine' 'Unbiased' 'Unclear' 'Biased against others'\n 'Biased against Israel' 'Biased against both Palestine and Israel'\n 'Not Applicable']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(df['Propaganda'].cat.categories if df['Propaganda'].dtype.name == 'category' else df['Propaganda'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.272936Z","iopub.execute_input":"2024-11-27T19:33:26.273168Z","iopub.status.idle":"2024-11-27T19:33:26.281731Z","shell.execute_reply.started":"2024-11-27T19:33:26.273141Z","shell.execute_reply":"2024-11-27T19:33:26.280924Z"}},"outputs":[{"name":"stdout","text":"['Not Propaganda' 'Propaganda' 'Unclear' 'Not Applicable']\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(df['Type of Propaganda'].cat.categories if df['Type of Propaganda'].dtype.name == 'category' else df['Type of Propaganda'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.282770Z","iopub.execute_input":"2024-11-27T19:33:26.283037Z","iopub.status.idle":"2024-11-27T19:33:26.292170Z","shell.execute_reply.started":"2024-11-27T19:33:26.283013Z","shell.execute_reply":"2024-11-27T19:33:26.291474Z"}},"outputs":[{"name":"stdout","text":"['Propaganda Not to be deleted' 'Not Propaganda'\n 'Propaganda Must be deleted' 'Propaganda May be deleted' 'Unclear'\n 'Not Applicable' 'Not propaganda' 'Propaganda May be deleted ' 'unclear'\n 'Propaganda']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"columns_to_check = ['Bias']\n\nfor column in columns_to_check:\n    print(f\"Class counts for '{column}':\")\n    print(df[column].value_counts())\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.293045Z","iopub.execute_input":"2024-11-27T19:33:26.293287Z","iopub.status.idle":"2024-11-27T19:33:26.306796Z","shell.execute_reply.started":"2024-11-27T19:33:26.293263Z","shell.execute_reply":"2024-11-27T19:33:26.305888Z"}},"outputs":[{"name":"stdout","text":"Class counts for 'Bias':\nBias\nUnbiased                                    6817\nBiased against Palestine                    2900\nUnclear                                      432\nBiased against Israel                        281\nBiased against others                        203\nNot Applicable                               120\nBiased against both Palestine and Israel      47\nName: count, dtype: int64\n\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"columns_to_check = ['Propaganda']\n\nfor column in columns_to_check:\n    print(f\"Class counts for '{column}':\")\n    print(df[column].value_counts())\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.307977Z","iopub.execute_input":"2024-11-27T19:33:26.308584Z","iopub.status.idle":"2024-11-27T19:33:26.319190Z","shell.execute_reply.started":"2024-11-27T19:33:26.308545Z","shell.execute_reply":"2024-11-27T19:33:26.318270Z"}},"outputs":[{"name":"stdout","text":"Class counts for 'Propaganda':\nPropaganda\nNot Propaganda    7098\nPropaganda        3301\nUnclear            269\nNot Applicable     132\nName: count, dtype: int64\n\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"columns_to_check = ['Type of Propaganda']\n\nfor column in columns_to_check:\n    print(f\"Class counts for '{column}':\")\n    print(df[column].value_counts())\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.320067Z","iopub.execute_input":"2024-11-27T19:33:26.320297Z","iopub.status.idle":"2024-11-27T19:33:26.331075Z","shell.execute_reply.started":"2024-11-27T19:33:26.320273Z","shell.execute_reply":"2024-11-27T19:33:26.330290Z"}},"outputs":[{"name":"stdout","text":"Class counts for 'Type of Propaganda':\nType of Propaganda\nNot Propaganda                  4192\nPropaganda Not to be deleted    2750\nPropaganda Must be deleted      1913\nPropaganda May be deleted       1349\nUnclear                          267\nNot propaganda                   185\nNot Applicable                   131\nPropaganda May be deleted          9\nunclear                            2\nPropaganda                         2\nName: count, dtype: int64\n\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install emoji","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:26.332104Z","iopub.execute_input":"2024-11-27T19:33:26.332426Z","iopub.status.idle":"2024-11-27T19:33:35.492154Z","shell.execute_reply.started":"2024-11-27T19:33:26.332390Z","shell.execute_reply":"2024-11-27T19:33:35.491011Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (2.13.2)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import re\nimport emoji\n\ndef preprocess_text(text):\n    # Ensure text is a string\n    text = str(text)\n    \n    # 1. Remove hashtags\n    text = re.sub(r'#\\w+', '', text)\n    \n    # 2. Remove full URLs with protocols (e.g., \"http://example.com\")\n    text = re.sub(r'https?://\\S+|www\\.\\S+|bit\\.ly\\S*', '', text)\n    \n    # 3. Remove standalone paths without protocols (e.g., \"/content/kan-news/defense/629514/\")\n    text = re.sub(r'/\\S+', '', text)\n    \n    # 4. Remove emails\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # 5. Remove emojis\n    text = emoji.replace_emoji(text, replace='')\n    \n    # 6. Remove diacritics\n    arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n    text = re.sub(arabic_diacritics, '', text)\n    \n    # 7. Remove Tatweel (ـ)\n    text = re.sub(r'ـ', '', text)\n    \n    # 8. Normalize Arabic text\n    text = re.sub(r'[إأآا]', 'ا', text)  # Unify Alif variants\n    text = re.sub(r'ة', 'ه', text)  # Replace Taa Marbuta with Haa\n    text = re.sub(r'ى', 'ي', text)  # Replace Alef Maqsura with Ya\n    \n    # 9. Remove repeated characters (e.g., \"ممتتتاز\" → \"ممتاز\")\n    text = re.sub(r'(.)\\1+', r'\\1', text)\n    \n    # Return cleaned text\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:35.493669Z","iopub.execute_input":"2024-11-27T19:33:35.493938Z","iopub.status.idle":"2024-11-27T19:33:35.524945Z","shell.execute_reply.started":"2024-11-27T19:33:35.493911Z","shell.execute_reply":"2024-11-27T19:33:35.524285Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Apply the preprocessing function\ndf['Arabic MT'] = df['Arabic MT'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:35.525994Z","iopub.execute_input":"2024-11-27T19:33:35.526315Z","iopub.status.idle":"2024-11-27T19:33:38.752083Z","shell.execute_reply.started":"2024-11-27T19:33:35.526273Z","shell.execute_reply":"2024-11-27T19:33:38.751222Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"df['Arabic MT'][9022]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:38.753035Z","iopub.execute_input":"2024-11-27T19:33:38.753289Z","iopub.status.idle":"2024-11-27T19:33:38.759612Z","shell.execute_reply.started":"2024-11-27T19:33:38.753264Z","shell.execute_reply":"2024-11-27T19:33:38.758707Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'اولا علي قناه ABC: جلست المرشحه الرئاسيه الجمهوريه نيكي هيلي مع لينسي ديفيس من قناه ABC News، حيث ناقشت مجموعه واسعه من المواضيع بما في ذلك الرئيس السابق. ترامب والحرب بين اسرائيل وحماس والاجهاض وحياتها قبل ان تظهر امام الجمهور. اقرا المزيد: '"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"!pip install transformers datasets torch arabert-preprocess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:38.760635Z","iopub.execute_input":"2024-11-27T19:33:38.760848Z","iopub.status.idle":"2024-11-27T19:33:40.604435Z","shell.execute_reply.started":"2024-11-27T19:33:38.760826Z","shell.execute_reply":"2024-11-27T19:33:40.603562Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n\u001b[31mERROR: Could not find a version that satisfies the requirement arabert-preprocess (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for arabert-preprocess\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"!pip install arabert","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:40.605777Z","iopub.execute_input":"2024-11-27T19:33:40.606046Z","iopub.status.idle":"2024-11-27T19:33:51.237414Z","shell.execute_reply.started":"2024-11-27T19:33:40.606020Z","shell.execute_reply":"2024-11-27T19:33:51.236558Z"}},"outputs":[{"name":"stdout","text":"Collecting arabert\n  Downloading arabert-1.0.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: PyArabic in /opt/conda/lib/python3.10/site-packages (from arabert) (0.6.15)\nCollecting farasapy (from arabert)\n  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\nCollecting emoji==1.4.2 (from arabert)\n  Downloading emoji-1.4.2.tar.gz (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from farasapy->arabert) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from farasapy->arabert) (4.66.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from PyArabic->arabert) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->farasapy->arabert) (2024.8.30)\nDownloading arabert-1.0.1-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading farasapy-0.0.14-py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186459 sha256=e3c1df75d783a040e4bbf219da7eb15895c57675d1214c089db521d4f75c354e\n  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\nSuccessfully built emoji\nInstalling collected packages: emoji, farasapy, arabert\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.13.2\n    Uninstalling emoji-2.13.2:\n      Successfully uninstalled emoji-2.13.2\nSuccessfully installed arabert-1.0.1 emoji-1.4.2 farasapy-0.0.14\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from arabert.preprocess import ArabertPreprocessor\n\n# Initialize the AraBERT preprocessor for the specific version of AraBERT\narabert_prep = ArabertPreprocessor(\"aubmindlab/bert-base-arabertv02\")\n\n# Apply preprocessing to the Arabic text column\ndf['Arabic MT'] = df['Arabic MT'].apply(arabert_prep.preprocess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:51.238919Z","iopub.execute_input":"2024-11-27T19:33:51.239286Z","iopub.status.idle":"2024-11-27T19:33:54.370810Z","shell.execute_reply.started":"2024-11-27T19:33:51.239243Z","shell.execute_reply":"2024-11-27T19:33:54.370154Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\n# Encode labels to integers\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['Bias'] = label_encoder.fit_transform(df['Bias'])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    df['Arabic MT'], df['Bias'], test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:54.372047Z","iopub.execute_input":"2024-11-27T19:33:54.372748Z","iopub.status.idle":"2024-11-27T19:33:58.086624Z","shell.execute_reply.started":"2024-11-27T19:33:54.372707Z","shell.execute_reply":"2024-11-27T19:33:58.085794Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class ArabicDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        # Tokenize and encode the input text\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:58.087715Z","iopub.execute_input":"2024-11-27T19:33:58.088145Z","iopub.status.idle":"2024-11-27T19:33:58.094060Z","shell.execute_reply.started":"2024-11-27T19:33:58.088118Z","shell.execute_reply":"2024-11-27T19:33:58.093168Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nMODEL_NAME = \"aubmindlab/bert-base-arabertv02\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:33:58.098428Z","iopub.execute_input":"2024-11-27T19:33:58.098718Z","iopub.status.idle":"2024-11-27T19:34:04.288195Z","shell.execute_reply.started":"2024-11-27T19:33:58.098691Z","shell.execute_reply":"2024-11-27T19:34:04.287543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/381 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a268b59cc12f4eff953f358ffe7447bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e2fcd420b34ea2ad1df24244d7e672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/825k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac7bd4d5a784d349ea231e99a663a9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.64M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee2673dcf734fa1b90c994f8ba9eea3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe8d07c1cd604ffbae25b71a1119a36e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefe1a8901a94d2a86292904cdbd80fb"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"MAX_LEN = 128\nBATCH_SIZE = 16\n\ntrain_dataset = ArabicDataset(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LEN)\ntest_dataset = ArabicDataset(X_test.tolist(), y_test.tolist(), tokenizer, MAX_LEN)\n\n\nfrom torch.utils.data import WeightedRandomSampler\n\n# Compute sample weights\nclass_sample_counts = y_train.value_counts().to_dict()\nweights = [1.0 / class_sample_counts[label] for label in y_train]\nsampler = WeightedRandomSampler(weights, len(weights))\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n\n\n#train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:34:04.289152Z","iopub.execute_input":"2024-11-27T19:34:04.289558Z","iopub.status.idle":"2024-11-27T19:34:04.300808Z","shell.execute_reply.started":"2024-11-27T19:34:04.289514Z","shell.execute_reply":"2024-11-27T19:34:04.299790Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\n\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, logits, labels):\n        ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction='none')\n        p_t = torch.exp(-ce_loss)\n        loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss\n        return loss.mean()\n\nloss_fn = FocalLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:34:04.301927Z","iopub.execute_input":"2024-11-27T19:34:04.302285Z","iopub.status.idle":"2024-11-27T19:34:04.319188Z","shell.execute_reply.started":"2024-11-27T19:34:04.302256Z","shell.execute_reply":"2024-11-27T19:34:04.318392Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\n\n# Define device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training loop\nEPOCHS = 40\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        # Compute loss\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Calculate training accuracy\n        _, predicted = torch.max(logits, dim=-1)\n        train_correct += (predicted == labels).sum().item()\n        train_total += labels.size(0)\n\n        train_loss += loss.item()\n\n        loop.set_description(f\"Epoch {epoch}\")\n        loop.set_postfix(loss=loss.item())\n\n    # Calculate training loss and accuracy\n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = 100 * train_correct / train_total\n    print(f\"Epoch {epoch} completed. Average train loss: {avg_train_loss:.4f}, Train accuracy: {train_accuracy:.2f}%\")\n\n    # Now evaluate on validation set (using test_loader)\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for batch in test_loader:  # Change val_loader to test_loader\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            # Forward pass\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            # Compute validation loss\n            loss = loss_fn(logits, labels)\n\n            # Calculate validation accuracy\n            _, predicted = torch.max(logits, dim=-1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n\n            val_loss += loss.item()\n\n    # Calculate validation loss and accuracy\n    avg_val_loss = val_loss / len(test_loader)  # Using test_loader for validation\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"Epoch {epoch} completed. Average val loss: {avg_val_loss:.4f}, Val accuracy: {val_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T19:34:04.320428Z","iopub.execute_input":"2024-11-27T19:34:04.320700Z","iopub.status.idle":"2024-11-27T20:53:04.103515Z","shell.execute_reply.started":"2024-11-27T19:34:04.320676Z","shell.execute_reply":"2024-11-27T20:53:04.102604Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 0: 100%|██████████| 540/540 [01:51<00:00,  4.86it/s, loss=0.44] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 completed. Average train loss: 0.9166, Train accuracy: 47.58%\nEpoch 0 completed. Average val loss: 0.8449, Val accuracy: 35.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.245] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed. Average train loss: 0.3292, Train accuracy: 75.91%\nEpoch 1 completed. Average val loss: 0.7918, Val accuracy: 37.50%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.144] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 completed. Average train loss: 0.1698, Train accuracy: 84.05%\nEpoch 2 completed. Average val loss: 0.6853, Val accuracy: 51.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0336]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 completed. Average train loss: 0.1305, Train accuracy: 85.65%\nEpoch 3 completed. Average val loss: 0.6962, Val accuracy: 56.25%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.291]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 completed. Average train loss: 0.0956, Train accuracy: 88.54%\nEpoch 4 completed. Average val loss: 0.7707, Val accuracy: 46.11%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.187]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 completed. Average train loss: 0.0945, Train accuracy: 88.09%\nEpoch 5 completed. Average val loss: 0.7557, Val accuracy: 56.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.0189] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 completed. Average train loss: 0.0780, Train accuracy: 89.63%\nEpoch 6 completed. Average val loss: 0.8366, Val accuracy: 44.40%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0803] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 completed. Average train loss: 0.0767, Train accuracy: 89.77%\nEpoch 7 completed. Average val loss: 0.8014, Val accuracy: 49.31%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.18]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 completed. Average train loss: 0.0695, Train accuracy: 90.21%\nEpoch 8 completed. Average val loss: 0.7958, Val accuracy: 54.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.00584] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 completed. Average train loss: 0.0622, Train accuracy: 91.70%\nEpoch 9 completed. Average val loss: 0.8383, Val accuracy: 52.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.0554] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 completed. Average train loss: 0.0607, Train accuracy: 91.91%\nEpoch 10 completed. Average val loss: 0.8518, Val accuracy: 52.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0227] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 completed. Average train loss: 0.0650, Train accuracy: 91.74%\nEpoch 11 completed. Average val loss: 0.8577, Val accuracy: 52.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0494] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 completed. Average train loss: 0.0569, Train accuracy: 92.77%\nEpoch 12 completed. Average val loss: 0.8882, Val accuracy: 54.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.00397] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 completed. Average train loss: 0.0525, Train accuracy: 93.09%\nEpoch 13 completed. Average val loss: 0.8688, Val accuracy: 59.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.0624]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 completed. Average train loss: 0.0494, Train accuracy: 93.99%\nEpoch 14 completed. Average val loss: 0.8943, Val accuracy: 58.80%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0664]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 completed. Average train loss: 0.0427, Train accuracy: 94.85%\nEpoch 15 completed. Average val loss: 0.9214, Val accuracy: 57.55%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0443] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 completed. Average train loss: 0.0410, Train accuracy: 95.01%\nEpoch 16 completed. Average val loss: 0.8997, Val accuracy: 58.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00909] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 completed. Average train loss: 0.0419, Train accuracy: 94.84%\nEpoch 17 completed. Average val loss: 0.9510, Val accuracy: 59.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00832] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 completed. Average train loss: 0.0491, Train accuracy: 94.66%\nEpoch 18 completed. Average val loss: 0.8975, Val accuracy: 57.45%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.0103]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 completed. Average train loss: 0.0311, Train accuracy: 96.23%\nEpoch 19 completed. Average val loss: 1.0013, Val accuracy: 60.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0316]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 completed. Average train loss: 0.0337, Train accuracy: 96.05%\nEpoch 20 completed. Average val loss: 1.0091, Val accuracy: 57.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.0462]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 completed. Average train loss: 0.0327, Train accuracy: 96.41%\nEpoch 21 completed. Average val loss: 1.0424, Val accuracy: 58.43%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 540/540 [01:50<00:00,  4.88it/s, loss=0.0692]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 completed. Average train loss: 0.0336, Train accuracy: 96.22%\nEpoch 22 completed. Average val loss: 1.0213, Val accuracy: 53.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00216] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 completed. Average train loss: 0.0281, Train accuracy: 96.71%\nEpoch 23 completed. Average val loss: 1.0346, Val accuracy: 53.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.0285]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 completed. Average train loss: 0.0287, Train accuracy: 96.82%\nEpoch 24 completed. Average val loss: 1.0121, Val accuracy: 58.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00352] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 completed. Average train loss: 0.0283, Train accuracy: 96.91%\nEpoch 25 completed. Average val loss: 0.9900, Val accuracy: 59.31%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.00137] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 completed. Average train loss: 0.0295, Train accuracy: 96.81%\nEpoch 26 completed. Average val loss: 1.0415, Val accuracy: 55.51%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0124]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 completed. Average train loss: 0.0238, Train accuracy: 97.47%\nEpoch 27 completed. Average val loss: 1.0418, Val accuracy: 54.68%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0202]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 completed. Average train loss: 0.0201, Train accuracy: 97.73%\nEpoch 28 completed. Average val loss: 1.1003, Val accuracy: 56.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.0218]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 completed. Average train loss: 0.0242, Train accuracy: 97.53%\nEpoch 29 completed. Average val loss: 1.1071, Val accuracy: 58.84%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00241] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 completed. Average train loss: 0.0246, Train accuracy: 97.40%\nEpoch 30 completed. Average val loss: 1.0902, Val accuracy: 51.16%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00119] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 31 completed. Average train loss: 0.0253, Train accuracy: 97.82%\nEpoch 31 completed. Average val loss: 1.0447, Val accuracy: 57.73%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00832] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 32 completed. Average train loss: 0.0236, Train accuracy: 98.04%\nEpoch 32 completed. Average val loss: 1.0897, Val accuracy: 59.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00345] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 33 completed. Average train loss: 0.0198, Train accuracy: 98.06%\nEpoch 33 completed. Average val loss: 1.1159, Val accuracy: 59.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00172] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 34 completed. Average train loss: 0.0185, Train accuracy: 98.06%\nEpoch 34 completed. Average val loss: 1.1512, Val accuracy: 57.50%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00219] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 35 completed. Average train loss: 0.0229, Train accuracy: 97.67%\nEpoch 35 completed. Average val loss: 1.0905, Val accuracy: 54.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.000377]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36 completed. Average train loss: 0.0193, Train accuracy: 98.11%\nEpoch 36 completed. Average val loss: 1.1840, Val accuracy: 59.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.000584]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37 completed. Average train loss: 0.0175, Train accuracy: 98.17%\nEpoch 37 completed. Average val loss: 1.1551, Val accuracy: 59.68%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 540/540 [01:50<00:00,  4.89it/s, loss=0.00835] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 38 completed. Average train loss: 0.0175, Train accuracy: 98.25%\nEpoch 38 completed. Average val loss: 1.1233, Val accuracy: 59.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 540/540 [01:50<00:00,  4.90it/s, loss=0.00101] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 39 completed. Average train loss: 0.0231, Train accuracy: 97.70%\nEpoch 39 completed. Average val loss: 1.0863, Val accuracy: 58.06%\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n\n# Classification report\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:53:04.110769Z","iopub.execute_input":"2024-11-27T20:53:04.111017Z","iopub.status.idle":"2024-11-27T20:53:12.193734Z","shell.execute_reply.started":"2024-11-27T20:53:04.110993Z","shell.execute_reply":"2024-11-27T20:53:12.192843Z"}},"outputs":[{"name":"stdout","text":"                                          precision    recall  f1-score   support\n\n                   Biased against Israel       0.04      0.02      0.03        56\n                Biased against Palestine       0.42      0.44      0.43       585\nBiased against both Palestine and Israel       0.00      0.00      0.00         7\n                   Biased against others       0.10      0.05      0.07        39\n                          Not Applicable       0.22      0.07      0.11        27\n                                Unbiased       0.67      0.74      0.70      1344\n                                 Unclear       0.00      0.00      0.00       102\n\n                                accuracy                           0.58      2160\n                               macro avg       0.21      0.19      0.19      2160\n                            weighted avg       0.54      0.58      0.56      2160\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model.save_pretrained(\"arabert_bias_classifier\")\ntokenizer.save_pretrained(\"arabert_bias_classifier\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:53:12.194813Z","iopub.execute_input":"2024-11-27T20:53:12.195077Z","iopub.status.idle":"2024-11-27T20:53:13.783983Z","shell.execute_reply.started":"2024-11-27T20:53:12.195051Z","shell.execute_reply":"2024-11-27T20:53:13.783145Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('arabert_bias_classifier/tokenizer_config.json',\n 'arabert_bias_classifier/special_tokens_map.json',\n 'arabert_bias_classifier/vocab.txt',\n 'arabert_bias_classifier/added_tokens.json',\n 'arabert_bias_classifier/tokenizer.json')"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}